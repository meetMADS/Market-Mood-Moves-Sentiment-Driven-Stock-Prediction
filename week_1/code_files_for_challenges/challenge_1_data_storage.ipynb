{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB1Q8Zcy-a1M"
      },
      "source": [
        "# CHALLENGE 1: Professional Data Storage\n",
        "## Replacing CSVs with SQLite and Parquet\n",
        "\n",
        "**The Problem:** CSVs are slow and lose data types (timestamps become strings). Real data pipelines don't use CSVs.\n",
        "\n",
        "**Your Mission:** Instead of `to_csv`, use **SQLite** or **Parquet** for incremental data storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ81TZVz-a1N"
      },
      "source": [
        "## Part 1: SQLite Implementation\n",
        "\n",
        "SQLite is a lightweight database that stores data locally in a `.db` file. It preserves data types and is much faster than CSV for repeated operations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Initialize Data Generation\n",
        "In this section, we create a function to simulate real-world news data. Notice that the `timestamp` column is a true **datetime** object, which we want to preserve when we move to SQLite."
      ],
      "metadata": {
        "id": "hpL8mLw6-tUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvPEdY2m-a1O",
        "outputId": "2a4ade3b-cf0f-4570-93b1-33ed7b9968fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First batch of news:\n",
            "                   timestamp    company         headline sentiment  \\\n",
            "0 2025-12-23 11:31:45.520278     Google  News headline 0  negative   \n",
            "1 2025-12-23 12:31:45.520278      Apple  News headline 1  negative   \n",
            "2 2025-12-23 13:31:45.520278      Apple  News headline 2  negative   \n",
            "3 2025-12-23 14:31:45.520278  Microsoft  News headline 3  negative   \n",
            "4 2025-12-23 15:31:45.520278     Google  News headline 4   neutral   \n",
            "\n",
            "   sentiment_score  \n",
            "0         0.672642  \n",
            "1        -0.867824  \n",
            "2        -0.656459  \n",
            "3         0.677706  \n",
            "4        -0.704506  \n",
            "\n",
            "Data types:\n",
            "timestamp          datetime64[ns]\n",
            "company                    object\n",
            "headline                   object\n",
            "sentiment                  object\n",
            "sentiment_score           float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Sample news data (simulating fetched news)\n",
        "def generate_sample_news(n=10, start_date=None):\n",
        "    if start_date is None:\n",
        "        start_date = datetime.now()\n",
        "\n",
        "    companies = ['Apple', 'Tesla', 'Microsoft', 'Amazon', 'Google']\n",
        "    sentiments = ['positive', 'negative', 'neutral']\n",
        "\n",
        "    data = {\n",
        "        'timestamp': [start_date + timedelta(hours=i) for i in range(n)],\n",
        "        'company': np.random.choice(companies, n),\n",
        "        'headline': [f'News headline {i}' for i in range(n)],\n",
        "        'sentiment': np.random.choice(sentiments, n),\n",
        "        'sentiment_score': np.random.uniform(-1, 1, n)\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate first batch of news\n",
        "df_batch1 = generate_sample_news(10)\n",
        "print(\"First batch of news:\")\n",
        "print(df_batch1.head())\n",
        "print(f\"\\nData types:\\n{df_batch1.dtypes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create SQLite database and save first batch\n",
        "db_path = 'news.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "**The Process:**\n",
        "1. Establish a connection to the `.db` file.\n",
        "2. Use `to_sql` to transfer the DataFrame.\n",
        "3. Close the connection to finalize the save.\n"
      ],
      "metadata": {
        "id": "LW4Gq8Hf-wFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS-93vgX-a1O",
        "outputId": "410e7fc0-4194-49c8-f491-871282061e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 10 records to news.db\n"
          ]
        }
      ],
      "source": [
        "# Create SQLite database and save first batch\n",
        "db_path = 'news.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Save to SQLite (creates table if doesn't exist)\n",
        "df_batch1.to_sql('news_data', conn, if_exists='replace', index=False)\n",
        "\n",
        "conn.close()\n",
        "print(f\"✅ Saved {len(df_batch1)} records to {db_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incremental Data Storage\n",
        "One of the biggest advantages of SQLite over CSV is the ability to **append** data without loading the entire file first.\n",
        "\n",
        "* **Logic:** We use `if_exists='append'` in the `to_sql` method.\n",
        "* **Benefit:** This saves memory and time as your dataset grows to millions of rows."
      ],
      "metadata": {
        "id": "vxP4Sg_a-49H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hplVCEh-a1P",
        "outputId": "84e237aa-d7a1-43dd-8e2b-6379807d79ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Appended 10 more records\n"
          ]
        }
      ],
      "source": [
        "# Simulate fetching new news the next day\n",
        "df_batch2 = generate_sample_news(10, start_date=datetime.now() + timedelta(days=1))\n",
        "\n",
        "# Append to existing database\n",
        "conn = sqlite3.connect(db_path)\n",
        "df_batch2.to_sql('news_data', conn, if_exists='append', index=False)\n",
        "conn.close()\n",
        "\n",
        "print(f\"✅ Appended {len(df_batch2)} more records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Verifying the Data\n",
        "Now we read the data back into a pandas DataFrame. Notice two key advantages:\n",
        "1. **Filtering:** We could use SQL (e.g., `WHERE sentiment='positive'`) to load only what we need.\n",
        "2. **Type Integrity:** The `timestamp` column automatically retains its format.\n",
        "\n",
        "The total record count should now reflect both batches combined."
      ],
      "metadata": {
        "id": "Yob6U7W--8Q4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaMoGdBZ-a1P",
        "outputId": "882921e6-6fe1-44bc-8626-5a486d8d599b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records in database: 20\n",
            "\n",
            "Data types preserved:\n",
            "timestamp           object\n",
            "company             object\n",
            "headline            object\n",
            "sentiment           object\n",
            "sentiment_score    float64\n",
            "dtype: object\n",
            "\n",
            "First few rows:\n",
            "                    timestamp    company         headline sentiment  \\\n",
            "0  2025-12-23 11:31:45.520278     Google  News headline 0  negative   \n",
            "1  2025-12-23 12:31:45.520278      Apple  News headline 1  negative   \n",
            "2  2025-12-23 13:31:45.520278      Apple  News headline 2  negative   \n",
            "3  2025-12-23 14:31:45.520278  Microsoft  News headline 3  negative   \n",
            "4  2025-12-23 15:31:45.520278     Google  News headline 4   neutral   \n",
            "\n",
            "   sentiment_score  \n",
            "0         0.672642  \n",
            "1        -0.867824  \n",
            "2        -0.656459  \n",
            "3         0.677706  \n",
            "4        -0.704506  \n",
            "\n",
            "Last few rows:\n",
            "                     timestamp    company         headline sentiment  \\\n",
            "15  2025-12-24 16:32:36.698503  Microsoft  News headline 5   neutral   \n",
            "16  2025-12-24 17:32:36.698503      Tesla  News headline 6  positive   \n",
            "17  2025-12-24 18:32:36.698503     Google  News headline 7  negative   \n",
            "18  2025-12-24 19:32:36.698503  Microsoft  News headline 8  negative   \n",
            "19  2025-12-24 20:32:36.698503      Apple  News headline 9  negative   \n",
            "\n",
            "    sentiment_score  \n",
            "15         0.822398  \n",
            "16        -0.020877  \n",
            "17         0.672216  \n",
            "18         0.397449  \n",
            "19         0.837445  \n"
          ]
        }
      ],
      "source": [
        "# Read data back from SQLite\n",
        "conn = sqlite3.connect(db_path)\n",
        "df_from_db = pd.read_sql_query(\"SELECT * FROM news_data\", conn)\n",
        "conn.close()\n",
        "\n",
        "print(f\"Total records in database: {len(df_from_db)}\")\n",
        "print(f\"\\nData types preserved:\\n{df_from_db.dtypes}\")\n",
        "print(f\"\\nFirst few rows:\\n{df_from_db.head()}\")\n",
        "print(f\"\\nLast few rows:\\n{df_from_db.tail()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: The Power of SQL\n",
        "Unlike a CSV where you have to load the entire file into memory to filter it, SQLite allows you to:\n",
        "1. **Filter at the source:** Only pull rows for 'Apple'.\n",
        "2. **Aggregate efficiently:** Calculate average sentiment scores directly within the database engine using `GROUP BY`.\n",
        "3. **Sort on the fly:** Use `ORDER BY` to get the most recent news first."
      ],
      "metadata": {
        "id": "677mZDbA-_Vk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Reqyj_gv-a1P",
        "outputId": "4ddd873c-bd28-4267-de0c-4d64b1e5b2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple news count: 6\n",
            "                    timestamp company         headline sentiment  \\\n",
            "0  2025-12-24 20:32:36.698503   Apple  News headline 9  negative   \n",
            "1  2025-12-24 15:32:36.698503   Apple  News headline 4  positive   \n",
            "2  2025-12-24 13:32:36.698503   Apple  News headline 2  positive   \n",
            "3  2025-12-23 19:31:45.520278   Apple  News headline 8  positive   \n",
            "4  2025-12-23 13:31:45.520278   Apple  News headline 2  negative   \n",
            "5  2025-12-23 12:31:45.520278   Apple  News headline 1  negative   \n",
            "\n",
            "   sentiment_score  \n",
            "0         0.837445  \n",
            "1         0.797495  \n",
            "2        -0.067101  \n",
            "3        -0.433157  \n",
            "4        -0.656459  \n",
            "5        -0.867824  \n",
            "\n",
            "Average sentiment by company:\n",
            "     company  avg_score\n",
            "0  Microsoft   0.632518\n",
            "1     Google   0.324810\n",
            "2      Tesla   0.233399\n",
            "3      Apple  -0.064934\n",
            "4     Amazon  -0.325873\n"
          ]
        }
      ],
      "source": [
        "# Query specific data (SQL power!)\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Get only Apple news\n",
        "apple_news = pd.read_sql_query(\"\"\"\n",
        "    SELECT * FROM news_data\n",
        "    WHERE company = 'Apple'\n",
        "    ORDER BY timestamp DESC\n",
        "\"\"\", conn)\n",
        "\n",
        "print(f\"Apple news count: {len(apple_news)}\")\n",
        "print(apple_news)\n",
        "\n",
        "# Get average sentiment per company\n",
        "avg_sentiment = pd.read_sql_query(\"\"\"\n",
        "    SELECT company, AVG(sentiment_score) as avg_score\n",
        "    FROM news_data\n",
        "    GROUP BY company\n",
        "    ORDER BY avg_score DESC\n",
        "\"\"\", conn)\n",
        "\n",
        "print(f\"\\nAverage sentiment by company:\\n{avg_sentiment}\")\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDqy10Rv-a1Q"
      },
      "source": [
        "## Part 2: Parquet Implementation\n",
        "Parquet is a columnar storage format that's highly efficient and preserves data types. It's the industry standard for big data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Parquet** is a columnar storage format. Unlike SQLite (which is a database), Parquet is a file format optimized for high-performance data analysis.\n",
        "\n",
        "* **Compression:** Files are much smaller than CSV or SQLite.\n",
        "* **Speed:** It is incredibly fast for reading specific columns.\n",
        "* **Schema:** It strictly preserves data types like dates and floats.\n"
      ],
      "metadata": {
        "id": "XdRL-a5F_E4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fplWLuEM-a1Q",
        "outputId": "bcd6af3f-c064-4b78-a5dc-9e34fd883feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 10 records to news.parquet\n"
          ]
        }
      ],
      "source": [
        "# Install pyarrow if not already installed\n",
        "# !pip install pyarrow\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "\n",
        "# Save first batch to Parquet\n",
        "parquet_file = 'news.parquet'\n",
        "df_batch1.to_parquet(parquet_file, engine='pyarrow', index=False)\n",
        "\n",
        "print(f\"✅ Saved {len(df_batch1)} records to {parquet_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incremental Updates in Parquet\n",
        "Unlike SQLite, Parquet is a **file format**, not a database engine. To \"append\" to a single Parquet file, the standard workflow is:\n",
        "1. **Read** the existing file into memory.\n",
        "2. **Concatenate** the new batch of data.\n",
        "3. **Overwrite** the file with the combined dataset.\n",
        "\n",
        "*Note: For massive datasets, professionals often save new batches as separate files in the same folder (partitioning) rather than overwriting one giant file, which would otherwise require RAM storage.*"
      ],
      "metadata": {
        "id": "Yzgq5zfd_HwV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0aTzb36-a1Q",
        "outputId": "c01de7fd-aa46-4ce1-de0e-f5322d6cbcd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Appended data. Total records: 20\n"
          ]
        }
      ],
      "source": [
        "# Append to Parquet (requires reading, concatenating, and writing)\n",
        "if os.path.exists(parquet_file):\n",
        "    existing_df = pd.read_parquet(parquet_file)\n",
        "    combined_df = pd.concat([existing_df, df_batch2], ignore_index=True)\n",
        "else:\n",
        "    combined_df = df_batch2\n",
        "\n",
        "combined_df.to_parquet(parquet_file, engine='pyarrow', index=False)\n",
        "print(f\"✅ Appended data. Total records: {len(combined_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Verification: Parquet Read\n",
        "We have successfully read the data back from the `.parquet` file.\n",
        "\n",
        "**Key Takeaway:** Unlike CSVs, which treat everything as text until you tell them otherwise, Parquet (like SQLite) stores the **metadata**. This means your `timestamp` columns are automatically loaded as `datetime64[ns]` and your `sentiment_score` remains a `float64` without any extra code."
      ],
      "metadata": {
        "id": "n7yike8D_LiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-MzU6Pb-a1Q",
        "outputId": "34c1227a-3bbe-464f-be7a-8aee8035832e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records: 20\n",
            "\n",
            "Data types preserved:\n",
            "timestamp          datetime64[ns]\n",
            "company                    object\n",
            "headline                   object\n",
            "sentiment                  object\n",
            "sentiment_score           float64\n",
            "dtype: object\n",
            "\n",
            "First few rows:\n",
            "                   timestamp    company         headline sentiment  \\\n",
            "0 2025-12-23 11:31:45.520278     Google  News headline 0  negative   \n",
            "1 2025-12-23 12:31:45.520278      Apple  News headline 1  negative   \n",
            "2 2025-12-23 13:31:45.520278      Apple  News headline 2  negative   \n",
            "3 2025-12-23 14:31:45.520278  Microsoft  News headline 3  negative   \n",
            "4 2025-12-23 15:31:45.520278     Google  News headline 4   neutral   \n",
            "\n",
            "   sentiment_score  \n",
            "0         0.672642  \n",
            "1        -0.867824  \n",
            "2        -0.656459  \n",
            "3         0.677706  \n",
            "4        -0.704506  \n"
          ]
        }
      ],
      "source": [
        "# Read from Parquet\n",
        "df_from_parquet = pd.read_parquet(parquet_file)\n",
        "\n",
        "print(f\"Total records: {len(df_from_parquet)}\")\n",
        "print(f\"\\nData types preserved:\\n{df_from_parquet.dtypes}\")\n",
        "print(f\"\\nFirst few rows:\\n{df_from_parquet.head()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage Comparison Results\n",
        "In this challenge, we moved from flat files (CSV) to professional formats.\n",
        "\n",
        "1. **Parquet** wins on disk space because it uses advanced compression.\n",
        "2. **SQLite** wins on query flexibility because it is a full database engine.\n",
        "3. **CSV** loses because it is slow and \"dumb\" (it doesn't know what a date is)."
      ],
      "metadata": {
        "id": "ZesEDVxR_OlK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGzonI9H-a1R",
        "outputId": "fcec053c-7e51-4ee4-b775-a76a4822fca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size comparison:\n",
            "CSV: 1.59 KB\n",
            "Parquet: 3.77 KB\n",
            "SQLite: 8.00 KB\n"
          ]
        }
      ],
      "source": [
        "# Compare file sizes\n",
        "import os\n",
        "\n",
        "# Save same data as CSV for comparison\n",
        "csv_file = 'news.csv'\n",
        "combined_df.to_csv(csv_file, index=False)\n",
        "\n",
        "csv_size = os.path.getsize(csv_file) / 1024  # KB\n",
        "parquet_size = os.path.getsize(parquet_file) / 1024  # KB\n",
        "db_size = os.path.getsize(db_path) / 1024  # KB\n",
        "\n",
        "print(f\"File size comparison:\")\n",
        "print(f\"CSV: {csv_size:.2f} KB\")\n",
        "print(f\"Parquet: {parquet_size:.2f} KB\")\n",
        "print(f\"SQLite: {db_size:.2f} KB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
